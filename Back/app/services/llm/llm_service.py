# AI Dock Main LLM Service - Refactored Thin Facade
# Simplified orchestration layer that delegates to modular components

from typing import Dict, Any, Optional, List, AsyncGenerator
import logging

from ...models.llm_config import LLMConfiguration
from .core.orchestrator import get_llm_orchestrator
from .models import ChatResponse
from .exceptions import LLMServiceError


class LLMService:
    """
    Main LLM service facade that provides a clean interface to LLM functionality.
    
    This service has been refactored from 1500+ lines to ~100 lines by extracting
    all business logic into modular atomic components. The service now acts as a
    thin facade that delegates all operations to the orchestrator.
    
    Architecture:
    - LLMService (this facade) â†’ LLMOrchestrator â†’ Atomic Components
    - All business logic is in atomic components (handlers, core, logging)
    - This service provides backward compatibility and a clean public API
    """
    
    def __init__(self):
        """Initialize the LLM service facade."""
        self.logger = logging.getLogger(__name__)
        self.orchestrator = get_llm_orchestrator()
        self.logger.info("LLM Service initialized as thin facade with modular orchestrator")
    
    # =============================================================================
    # MAIN CHAT REQUEST METHODS (Delegate to Orchestrator)
    # =============================================================================
    
    async def send_chat_request(
        self,
        config_id: int,
        messages: List[Dict[str, str]],
        user_id: int,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        session_id: Optional[str] = None,
        request_id: Optional[str] = None,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
        assistant_id: Optional[int] = None,
        bypass_quota: bool = False,
        **kwargs
    ) -> ChatResponse:
        """
        Send a regular chat request.
        
        This method provides backward compatibility while delegating all
        processing to the modular orchestrator.
        
        Args:
            config_id: ID of the LLM configuration to use
            messages: List of message dictionaries (role, content)
            user_id: ID of user making the request (for usage tracking)
            model: Override model (optional)
            temperature: Override temperature (optional)
            max_tokens: Override max tokens (optional)
            session_id: Session identifier for grouping requests (optional)
            request_id: Unique request identifier for tracing (optional)
            ip_address: Client IP address (optional)
            user_agent: Client user agent string (optional)
            assistant_id: ID of custom assistant being used (optional)
            bypass_quota: If True, skip quota checking (admin only)
            **kwargs: Additional provider-specific parameters
            
        Returns:
            Chat response from the LLM
            
        Raises:
            LLMServiceError: If request fails at any stage
        """
        self.logger.debug(f"LLM Service delegating chat request to orchestrator - User: {user_id}")
        
        return await self.orchestrator.process_chat_request(
            config_id=config_id,
            messages=messages,
            user_id=user_id,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            session_id=session_id,
            request_id=request_id,
            ip_address=ip_address,
            user_agent=user_agent,
            assistant_id=assistant_id,
            bypass_quota=bypass_quota,
            **kwargs
        )
    
    async def stream_chat_request(
        self,
        config_id: int,
        messages: List[Dict[str, str]], 
        user_id: int,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        session_id: Optional[str] = None,
        request_id: Optional[str] = None,
        ip_address: Optional[str] = None,
        user_agent: Optional[str] = None,
        bypass_quota: bool = False,
        **kwargs
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Stream a chat request with real-time response chunks.
        
        This method provides backward compatibility while delegating all
        streaming processing to the modular orchestrator.
        
        Args:
            config_id: ID of the LLM configuration to use
            messages: List of message dictionaries (role, content)
            user_id: ID of user making the request (for usage tracking)
            model: Override model (optional)
            temperature: Override temperature (optional)
            max_tokens: Override max tokens (optional)
            session_id: Session identifier for grouping requests (optional)
            request_id: Unique request identifier for tracing (optional)
            ip_address: Client IP address (optional)
            user_agent: Client user agent (optional)
            bypass_quota: If True, skip quota checking (admin only)
            **kwargs: Additional provider-specific parameters
            
        Yields:
            Dict[str, Any]: Streaming chunks with content, metadata, and final stats
        """
        self.logger.debug(f"LLM Service delegating streaming request to orchestrator - User: {user_id}")
        
        async for chunk in self.orchestrator.process_streaming_request(
            config_id=config_id,
            messages=messages,
            user_id=user_id,
            model=model,
            temperature=temperature,
            max_tokens=max_tokens,
            session_id=session_id,
            request_id=request_id,
            ip_address=ip_address,
            user_agent=user_agent,
            bypass_quota=bypass_quota,
            **kwargs
        ):
            yield chunk
    
    # =============================================================================
    # CONFIGURATION AND TESTING METHODS (Delegate to Orchestrator)
    # =============================================================================
    
    async def test_configuration(
        self, 
        config: Optional[LLMConfiguration] = None,
        config_id: Optional[int] = None,
        test_message: str = "Hello! This is a test.",
        timeout_seconds: int = 30
    ) -> Dict[str, Any]:
        """
        Test a specific LLM configuration.
        
        Delegates to orchestrator for actual testing while providing
        backward compatibility.
        
        Args:
            config: LLM configuration object (optional)
            config_id: Configuration ID if config not provided
            test_message: Test message to send
            timeout_seconds: Test timeout
            
        Returns:
            Test result dictionary
        """
        self.logger.debug(f"LLM Service delegating config test to orchestrator - Config: {config_id}")
        
        return await self.orchestrator.test_configuration(
            config=config,
            config_id=config_id,
            test_message=test_message,
            timeout_seconds=timeout_seconds
        )
    
    async def estimate_request_cost(
        self,
        config_id: int,
        messages: List[Dict[str, str]],
        model: Optional[str] = None,
        max_tokens: Optional[int] = None
    ) -> Optional[float]:
        """
        Estimate the cost of a chat request.
        
        Delegates to orchestrator for cost estimation.
        
        Args:
            config_id: ID of the LLM configuration
            messages: List of message dictionaries
            model: Optional model override
            max_tokens: Optional max tokens override
            
        Returns:
            Estimated cost in USD, or None if estimation not possible
        """
        self.logger.debug(f"LLM Service delegating cost estimation to orchestrator - Config: {config_id}")
        
        return await self.orchestrator.estimate_request_cost(
            config_id=config_id,
            messages=messages,
            model=model,
            max_tokens=max_tokens
        )
    
    # =============================================================================
    # QUOTA STATUS METHODS (Delegate to Orchestrator)
    # =============================================================================
    
    async def get_user_quota_status(self, user_id: int) -> Dict[str, Any]:
        """
        Get quota status for a user's department.
        
        Delegates to orchestrator for quota status checking.
        
        Args:
            user_id: User ID to check quota for
            
        Returns:
            Quota status information
        """
        self.logger.debug(f"LLM Service delegating quota status check to orchestrator - User: {user_id}")
        
        return await self.orchestrator.get_user_quota_status(user_id)
    
    async def check_user_can_use_config(self, user_id: int, config_id: int) -> Dict[str, Any]:
        """
        Check if a user can use a specific LLM configuration based on quotas.
        
        Delegates to orchestrator for configuration usage checking.
        
        Args:
            user_id: User ID to check
            config_id: Configuration ID to check
            
        Returns:
            Configuration usage permission information
        """
        self.logger.debug(f"LLM Service delegating config usage check to orchestrator - User: {user_id}, Config: {config_id}")
        
        return await self.orchestrator.check_user_can_use_config(user_id, config_id)
    
    # =============================================================================
    # SERVICE STATUS AND HEALTH METHODS
    # =============================================================================
    
    def get_service_status(self) -> Dict[str, Any]:
        """
        Get status of the LLM service and all its components.
        
        Returns:
            Comprehensive status information for monitoring
        """
        orchestrator_status = self.orchestrator.get_orchestrator_status()
        
        return {
            "service": "healthy",
            "architecture": "modular_facade",
            "facade_version": "refactored_thin",
            "available_operations": self.orchestrator.get_available_operations(),
            "orchestrator_status": orchestrator_status,
            "components_count": len(orchestrator_status.get("components", {})),
            "backward_compatibility": "maintained"
        }
    
    def get_available_methods(self) -> List[str]:
        """
        Get list of available service methods for API documentation.
        
        Returns:
            List of method names available in this service
        """
        return [
            "send_chat_request",
            "stream_chat_request",
            "test_configuration", 
            "estimate_request_cost",
            "get_user_quota_status",
            "check_user_can_use_config",
            "get_service_status",
            "get_available_methods"
        ]

    async def get_dynamic_models(
        self,
        config_id: int,
        db,
        use_cache: bool = True,
        show_all_models: bool = False
    ) -> dict:
        from app.models.llm_config import LLMConfiguration
        from app.services.llm.core.orchestrator import LLMOrchestrator
        from app.services.model_filter import ModelFilterLevel

        config = await db.get(LLMConfiguration, config_id)
        if not config:
            raise ValueError(f"LLM config {config_id} not found")

        provider = config.provider.value if hasattr(config.provider, "value") else str(config.provider)
        default_model = config.default_model

        # Get orchestrator instance
        orchestrator = LLMOrchestrator()
        
        try:
            # ðŸš€ NEW: Check cache first if caching is enabled
            if use_cache:
                try:
                    from app.services.llm.cache import get_model_cache_manager
                    cache_manager = get_model_cache_manager()
                    
                    # Try to get cached models
                    cached_data = await cache_manager.get_models(
                        config_id=config_id,
                        show_all_models=show_all_models
                    )
                    
                    if cached_data:
                        self.logger.info(f"ðŸ“‹ Cache HIT: Returning cached models for config {config_id} (filtering: {not show_all_models})")
                        return {
                            "models": cached_data.models,
                            "provider": cached_data.provider,
                            "default_model": cached_data.default_model,
                            "cached": True,
                            "cached_at": cached_data.cached_at.isoformat(),
                            "expires_at": cached_data.expires_at.isoformat(),
                            "filtering_applied": cached_data.filtering_applied,
                            "total_api_models": cached_data.total_api_models,
                            "original_total_models": cached_data.original_total_models
                        }
                    else:
                        self.logger.info(f"ðŸ“‹ Cache MISS: Will fetch fresh models for config {config_id}")
                        
                except Exception as cache_error:
                    self.logger.warning(f"âš ï¸ Cache error, will fetch fresh models: {str(cache_error)}")
            
            # ðŸŒ Fetch fresh models from provider
            self.logger.info(f"ðŸ” Fetching fresh models from {provider} API for config {config_id}")
            self.logger.info(f"ðŸ” show_all_models={show_all_models}, use_cache={use_cache}")
            
            # Fetch models from the provider using the orchestrator
            # The orchestrator now handles caching internally for the full model list
            all_models = await orchestrator.get_provider_models(config_id)
            
            self.logger.info(f"âœ… Successfully fetched {len(all_models)} models from {provider} API: {all_models[:5]}...")
            
            # Apply filtering if not showing all models
            original_count = len(all_models)
            filtered_models = all_models
            
            if not show_all_models:
                from app.services.model_filter import OpenAIModelFilter, ModelFilterLevel
                filter_engine = OpenAIModelFilter()
                filtered_models, metadata = filter_engine.filter_models(
                    all_models, 
                    ModelFilterLevel.RECOMMENDED
                )
                self.logger.info(f"ðŸ” Applied filtering: {len(all_models)} -> {len(filtered_models)} models")
            
            # ðŸš€ NEW: Cache the filtered results if caching is enabled
            if use_cache and not show_all_models:
                try:
                    cache_success = await cache_manager.set_models(
                        config_id=config_id,
                        models=filtered_models,
                        provider=provider,
                        config_name=config.name,
                        default_model=default_model,
                        show_all_models=show_all_models,
                        ttl_seconds=3600,  # 1 hour cache
                        total_api_models=original_count,
                        original_total_models=original_count
                    )
                    
                    if cache_success:
                        self.logger.info(f"ðŸ’¾ Cached filtered models for config {config_id} (filtered: {len(filtered_models)} of {original_count})")
                    
                except Exception as cache_error:
                    self.logger.error(f"âŒ Error caching filtered models: {str(cache_error)}")
            
            return {
                "models": filtered_models,
                "provider": provider,
                "default_model": default_model,
                "cached": False,  # This is fresh data
                "filtering_applied": not show_all_models,
                "total_api_models": original_count,
                "original_total_models": original_count if not show_all_models else None
            }
            
        except Exception as e:
            # ðŸ› DEBUG: Add detailed error logging to see what's failing
            self.logger.error(f"âŒ Failed to fetch models from {provider} API for config {config_id}: {type(e).__name__}: {str(e)}")
            self.logger.error(f"âŒ Exception details: {repr(e)}")
            
            # Fallback to configuration models if API fails
            fallback_models = [default_model] if default_model else []
            self.logger.warning(f"ðŸ”„ Falling back to configuration models for config {config_id}: {fallback_models}")
            
            return {
                "models": fallback_models,
                "provider": provider,
                "default_model": default_model,
                "cached": False,
                "filtering_applied": False,
                "error": str(e)
            }


# Global service instance (singleton pattern - maintains backward compatibility)
_llm_service = None

def get_llm_service() -> LLMService:
    """
    Get the global LLM service instance.
    
    This maintains backward compatibility with the existing singleton pattern
    while providing access to the refactored modular service.
    
    Returns:
        Singleton LLM service instance (now a thin facade)
    """
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service


# Export main service (maintains backward compatibility)
__all__ = ['LLMService', 'get_llm_service']
